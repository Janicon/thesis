{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95fcf88",
   "metadata": {},
   "source": [
    "# Variables to set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9623f5",
   "metadata": {},
   "source": [
    "`standard_load_dir`: Relative save folder of the standard setup model\n",
    "\n",
    "`main_load_dir`: Relative save folder of the sub model in the hierarchical setup\n",
    "\n",
    "`sub_load_dir`: Relative save folder of the main model in the hierarchical setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d91adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_load_dir = './Saved Models/RoBERTa-standard'\n",
    "main_load_dir = './Saved Models/RoBERTa-main'\n",
    "sub_load_dir = './Saved Models/RoBERTa-sub'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f6793",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Data Filtering, Token Classification , Cleaning, Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27aea8c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import required libraries, download necessary data, open and save additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a785461b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['manny',\n",
       " 'pacquiao',\n",
       " 'mannypacquiao',\n",
       " 'bongbong',\n",
       " 'marcos',\n",
       " 'bongbongmarcos',\n",
       " 'faisal',\n",
       " 'mangondato',\n",
       " 'faisalmangondato',\n",
       " 'panfilo',\n",
       " 'lacson',\n",
       " 'panfilolacson',\n",
       " 'norberto',\n",
       " 'gonzales',\n",
       " 'norbertogonzales',\n",
       " 'leody',\n",
       " 'de guzman',\n",
       " 'leodydeguzman',\n",
       " 'guzman',\n",
       " 'ernesto',\n",
       " 'abella',\n",
       " 'ernestoabella',\n",
       " 'isko',\n",
       " 'moreno',\n",
       " 'iskomoreno',\n",
       " 'leni',\n",
       " 'robredo',\n",
       " 'lenirobredo',\n",
       " 'jose',\n",
       " 'montemayor',\n",
       " 'josemontemayor',\n",
       " 'yorme',\n",
       " 'lbm',\n",
       " 'vyvym',\n",
       " 'bybym',\n",
       " 'babym',\n",
       " 'veeveem',\n",
       " 'beebeem',\n",
       " '88m',\n",
       " 'partido lakas ng masa',\n",
       " 'plm',\n",
       " 'partido demokratiko sosyalista ng pilipinas',\n",
       " 'pdsp',\n",
       " 'katipunan ng kamalayang kayumanggi',\n",
       " 'partido federal ng pilipinas',\n",
       " 'pfp',\n",
       " 'dpp',\n",
       " 'democratic party of the philippines',\n",
       " 'aksyon demokratiko',\n",
       " 'aksyon',\n",
       " 'promdi',\n",
       " 'unity',\n",
       " 'uniteam',\n",
       " 'rosas',\n",
       " 'pinklawan']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import contractions\n",
    "import simplemma\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#tTagalog Stop words dictionary\n",
    "tl_stopwords_file = open('./preprocessing_files/stopwords-tl.txt', 'r')\n",
    "data = tl_stopwords_file.read()\n",
    "tl_stopwords_list = data.split('\\n')\n",
    "tl_stopwords_file.close()\n",
    "\n",
    "# opening the candidate file in read mode\n",
    "candidate_file = open('./preprocessing_files/candidate_keywords.txt', 'r')\n",
    "data = candidate_file.read()\n",
    "candidate_keywords = data.split('\\n')\n",
    "candidate_file.close()\n",
    "\n",
    "# opening the party file in read mode\n",
    "party_file = open('./preprocessing_files/party_keywords.txt', 'r')\n",
    "data = party_file.read()\n",
    "party_keywords = data.split('\\n')\n",
    "party_file.close()\n",
    "\n",
    "# opening the supporter file in read mode\n",
    "supporter_file = open('./preprocessing_files/supporter_keywords.txt', 'r')\n",
    "data = supporter_file.read()\n",
    "supporter_keywords = data.split('\\n')\n",
    "supporter_file.close()\n",
    "\n",
    "# opening the related file in read mode\n",
    "related_file = open('./preprocessing_files/related_keywords.txt', 'r')\n",
    "data = related_file.read()\n",
    "related_keywords = data.split('\\n')\n",
    "related_file.close()\n",
    "\n",
    "# opening the candidate file in read mode\n",
    "not_file = open('./preprocessing_files/not_keywords.txt', 'r')\n",
    "data = not_file.read()\n",
    "not_keywords = data.split('\\n')\n",
    "not_file.close()\n",
    "\n",
    "keywords = candidate_keywords + party_keywords\n",
    "keywords = list(map(lambda x: x.lower().strip(), keywords))\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260a4a8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The following are functions required for Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625bece6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fix common tagalog contractions\n",
    "def decontracted(phrase):\n",
    "    # specific (seen frequently in tweet comments)\n",
    "    phrase = re.sub(r\"ika\\'y\", \"ikaw ay\", phrase)\n",
    "    phrase = re.sub(r\"ba\\'t\", \"bakit\", phrase)\n",
    "    phrase = re.sub(r\"nya\", \"niya\", phrase)\n",
    "    phrase = re.sub(r\"nyo\", \"ninyo\", phrase)\n",
    "    phrase = re.sub(r\"kundi\", \"kung hindi\", phrase)\n",
    "    phrase = re.sub(r\" musta \", \"kumusta\", phrase)\n",
    "\n",
    "    # general (starts with ') [https://en.wiktionary.org/wiki/Category:Tagalog_contractions]\n",
    "    phrase = re.sub(r\"\\'yo\", \" iyo\", phrase)\n",
    "    phrase = re.sub(r\"\\'to\", \" ito\", phrase)\n",
    "    phrase = re.sub(r\"\\'di\", \" hindi\", phrase)\n",
    "    phrase = re.sub(r\"\\'no\", \" ano\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" at\", phrase)\n",
    "    phrase = re.sub(r\"\\'y\", \" ay\", phrase)\n",
    "    phrase = re.sub(r\"\\'ng\", \" ang\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3127db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# removes same consecutive emojis into one\n",
    "def remove_duplicate_emoji(orig_str):\n",
    "    prev_emoji = None\n",
    "    remove_duplicate_emoji = []\n",
    "    for c in orig_str:\n",
    "        if c in emoji.EMOJI_DATA:\n",
    "            if prev_emoji == c:\n",
    "                continue\n",
    "            prev_emoji = c\n",
    "        remove_duplicate_emoji.append(c)\n",
    "    return \"\".join(remove_duplicate_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef94fad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove non-white spaces of the labeled keywords\n",
    "def remove_chars_keep_uppercase(string):\n",
    "    pattern = r'(https?://\\w*[A-Z]\\w*|@\\w*[A-Z]\\w*|#\\w*[A-Z]\\w*)'\n",
    "    matches = re.findall(pattern, string)\n",
    "    for match in matches:\n",
    "        new_match = re.sub('[^A-Z]+', ' ', match)\n",
    "        string = string.replace(match, new_match)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69614f9b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# removes same consecutive labeled keywords into one\n",
    "def remove_duplicate_uppercase(text):\n",
    "    pattern = r'\\b([A-Z]+)(?:\\s+\\1)+\\b'\n",
    "    replace = r'\\1'\n",
    "    return re.sub(pattern, replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc941b5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# adds white space between different keywords\n",
    "def add_whitespace_to_keywords(text):\n",
    "    # Define the keywords\n",
    "    keywords = ['CANDIDATE', 'RELATED', 'SUPPORTER', 'PARTY']\n",
    "\n",
    "    # Create a regular expression pattern to match the keywords\n",
    "    pattern = re.compile('|'.join(keywords))\n",
    "\n",
    "    # Define a function to add whitespace between the matched keywords\n",
    "    def add_whitespace(match):\n",
    "        return ' ' + match.group(0)\n",
    "\n",
    "    # Use the sub() method to replace the matched keywords with whitespace\n",
    "    result = pattern.sub(add_whitespace, text)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ccca882",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Removes extra non-capital letters or digits after capital letters \n",
    "def remove_extra_trailing_chars(text):\n",
    "    # Define a regular expression pattern to match non-capital letters or digits after capital letters\n",
    "    pattern = re.compile(r'(?<=[A-Z])[^A-Z\\d\\s]+')\n",
    "\n",
    "    # Define a function to remove non-capital letters or digits after capital letters in a string\n",
    "    def replace_extra_chars(match):\n",
    "        return ''\n",
    "\n",
    "    # Use the sub() method to remove the extra characters\n",
    "    result = pattern.sub(replace_extra_chars, text)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8d0fbc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# label the keywords based on what category it belongs to\n",
    "def label_keywords(comment):\n",
    "    for index, keyword in enumerate(keywords):\n",
    "        if keyword == 'ong':\n",
    "            keyword = ' ong'\n",
    "        isKeyword = True\n",
    "        compiled = re.compile(re.escape(keyword), re.IGNORECASE)\n",
    "        for not_keyword in not_keywords:\n",
    "            if keyword in not_keyword and not_keyword in comment:\n",
    "                isKeyword = False\n",
    "                break\n",
    "        # label based on order of priority \n",
    "        if isKeyword:\n",
    "            if index < len(candidate_keywords): \n",
    "                comment = compiled.sub(\"CANDIDATE\", comment)\n",
    "                if re.search(r'[CANDIDATE][es]|[s]$', comment):\n",
    "                    comment = compiled.sub(\"RELATED\", comment)\n",
    "            elif index < (len(candidate_keywords) + len(party_keywords)):\n",
    "                comment = compiled.sub(\"PARTY\", comment)\n",
    "            elif index < (len(candidate_keywords) + len(party_keywords) + len(supporter_keywords)):\n",
    "                comment = compiled.sub(\"SUPPORTER\", comment)\n",
    "            else:\n",
    "                comment = compiled.sub(\"RELATED\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6424075a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# removes stopwords for both english and tagalog\n",
    "def remove_stopwords(text, language='english'):\n",
    "    if language == 'english':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    elif language == 'tagalog':\n",
    "        stop_words = tl_stopwords_list\n",
    "    else:\n",
    "        print('Unsupported language')\n",
    "        return text\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b07cede2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# [corner case] adds space between textualized emojis\n",
    "def add_space(string):\n",
    "    output_str = re.sub(r'(?<=[a-z0-9:])(?=[A-Z])|(?<=[A-Z])(?=[a-z0-9:])', ' ', string)\n",
    "    output_str = re.sub(r'::', ': :', output_str)\n",
    "    return output_str\n",
    "# [corner case] used in fixer only\n",
    "def remove_hashtag_word(string):\n",
    "    pattern = r\"HASHTAG\\s*\\S+\"\n",
    "    string = re.sub(pattern, \"HASHTAG\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e33f60",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Phase 1 and Phase 2 Functions For Data Preprocessing Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608edd9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "punctuations = '!\"$%&\\'()*+,-./;<=>?@[\\\\]^_`{|}~'\n",
    "transtab = str.maketrans(dict.fromkeys(punctuations, \"\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Used for example app, the dataset will undergo filtering Tagalog tweets,canonicalizing phrases, converting to lowercase, \n",
    "#identifying keywords, tokenizing links, mentions and hashtags, and shortening repeated emojis.\n",
    "def preprocess_phase_1_and_2(current_text):\n",
    "    # canonicalize sentence\n",
    "    current_text = unicodedata.normalize('NFKD', current_text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lower case\n",
    "    current_text = current_text.lower()\n",
    "    # Remove Keywords\n",
    "    current_text = label_keywords(current_text)\n",
    "    # Clean Keywords\n",
    "    current_text = remove_chars_keep_uppercase(current_text)\n",
    "    # Remove Links\n",
    "    current_text = re.sub(r'http\\S+', 'LINK', current_text)\n",
    "    # Remove Mentions\n",
    "    current_text = re.sub('@\\S+', 'MENTION', current_text)\n",
    "    # Remove Hashtag\n",
    "    current_text = re.sub('#\\S+', 'HASHTAG', current_text)\n",
    "    # Reduce Duplicated Emojis\n",
    "    current_text = remove_duplicate_emoji(current_text)\n",
    "    # Emojis to text\n",
    "    current_text = emoji.demojize(current_text)\n",
    "    # Add Space for Keywords\n",
    "    current_text = add_whitespace_to_keywords(current_text)\n",
    "    current_text = add_space(current_text)\n",
    "    # Remove Extra spaces\n",
    "    current_text = re.sub(r'\\s+', ' ', current_text.strip())\n",
    "    # Remove Extra charcters\n",
    "    current_text = remove_extra_trailing_chars(current_text)\n",
    "    # Remove Duplicate Keywords\n",
    "    current_text = remove_duplicate_uppercase(current_text)\n",
    "    # Remove tagalog Contractions\n",
    "    current_text = decontracted(current_text)\n",
    "    # Remove english Contractions\n",
    "    current_text = contractions.fix(current_text)\n",
    "    # Fix Elongated\n",
    "    current_text = re.sub(r'(\\w)(\\1{2,})', r'\\1\\1', current_text)\n",
    "    # Lemmatization for english only\n",
    "    current_text = lemmatizer.lemmatize(current_text)\n",
    "    # Remove Punctuations\n",
    "    current_text = current_text.translate(transtab)\n",
    "    # Remove stop words\n",
    "    current_text = remove_stopwords(current_text, language='english')\n",
    "    # Remove stop words\n",
    "    current_text = remove_stopwords(current_text, language='tagalog')\n",
    "    # Remove Extra spaces\n",
    "    current_text = \" \".join(current_text.split())\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aad437a",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#example\n",
    "pre_text = \"AkO NA nga BA sir BongBong?\"\n",
    "keywords = keywords + supporter_keywords + related_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bc12c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The process below is for Phase 1 and 2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1274a4fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nga ba sir CANDIDATE'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = preprocess_phase_1_and_2(pre_text)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e3be8",
   "metadata": {},
   "source": [
    "# RoBERTa Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f41580",
   "metadata": {},
   "source": [
    "## Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cad5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\.conda\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dataset prepration\n",
    "#from transformers import TFAutoModel, AutoModel\n",
    "from transformers import AutoTokenizer, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model, hyperparameter search, evaluation\n",
    "import torch\n",
    "from transformers import BertPreTrainedModel, TrainingArguments\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import evaluate\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b887ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables for indexing\n",
    "classes = [\"Explicit\", \"Implicit\", \"Non-abusive\"]\n",
    "labels = [\"E1\", \"E2\", \"E3\", \"I1\", \"I2\", \"I3\", \"I4\", \"I5\", \"I6\", \"I7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb39a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(dataset, use_stopwords):\n",
    "    if dataset not in [\"train\", \"validate\", \"test\"]:\n",
    "        raise Exception(\"Invalid split.\")\n",
    "    if type(use_stopwords) != bool:\n",
    "        raise Exception(\"Stop words must be specified in boolean.\")\n",
    "    \n",
    "    stopwords = \"With Stopwords\"\n",
    "    if not use_stopwords:\n",
    "        stopwords = \"Without Stopwords\"\n",
    "    df = pd.read_csv(f\"./Data/{stopwords}/{dataset}.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5996a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(df):\n",
    "    df.rename({\"Text\": \"text\", \"Class\": \"labels\"}, axis=1, inplace=True)\n",
    "    df.drop(labels, axis=1, inplace=True)\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Convert pd labels to huggingface ClassLabels for stratifying\n",
    "    dataset = dataset.class_encode_column(\"labels\")\n",
    "    \n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Convert datasets to pytorch format\n",
    "    dataset = dataset.remove_columns([\"text\"])\n",
    "    dataset.set_format(\"torch\")\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "228189b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "load_dir = \"jcblaise/roberta-tagalog-base\"\n",
    "\n",
    "# Get tokenizer from repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_dir, model_max_length=256)\n",
    "\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8652d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classification head for pretrained RoBERTa\n",
    "class RobertaAbusiveClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(RobertaAbusiveClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af13ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    global predictions\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    pre_metric = evaluate.load(\"precision\")\n",
    "    rec_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    \n",
    "    return {\"f1\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e60fea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaAbusiveClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set configurations\n",
    "standard_num_labels=3\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "standard_config = RobertaConfig.from_pretrained(standard_load_dir, num_labels=standard_num_labels)\n",
    "standard_model = RobertaAbusiveClassification.from_pretrained(standard_load_dir, config=standard_config)\n",
    "standard_model.to(device)\n",
    "\n",
    "standard_training_args = TrainingArguments(\n",
    "    output_dir= 'predict',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    report_to = \"none\")\n",
    "\n",
    "standard_trainer = Trainer(\n",
    "    model = standard_model,\n",
    "    args = standard_training_args,\n",
    "    compute_metrics = compute_metrics)\n",
    "\n",
    "standard_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "111016ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaAbusiveClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set configurations\n",
    "num_labels=2\n",
    "\n",
    "main_config = RobertaConfig.from_pretrained(main_load_dir, num_labels=num_labels)\n",
    "main_model = RobertaAbusiveClassification.from_pretrained(main_load_dir, config=main_config)\n",
    "main_model.to(device)\n",
    "\n",
    "main_training_args = TrainingArguments(\n",
    "    output_dir= 'predict',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    report_to = \"none\")\n",
    "\n",
    "main_trainer = Trainer(\n",
    "    model = main_model,\n",
    "    args = main_training_args,\n",
    "    compute_metrics = compute_metrics)\n",
    "\n",
    "main_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "621e4b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaAbusiveClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set configurations\n",
    "num_labels=2\n",
    "\n",
    "sub_config = RobertaConfig.from_pretrained(sub_load_dir, num_labels=num_labels)\n",
    "sub_model = RobertaAbusiveClassification.from_pretrained(sub_load_dir, config=sub_config)\n",
    "sub_model.to(device)\n",
    "\n",
    "sub_training_args = TrainingArguments(\n",
    "    output_dir= 'predict',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    report_to = \"none\")\n",
    "\n",
    "sub_trainer = Trainer(\n",
    "    model = sub_model,\n",
    "    args = sub_training_args,\n",
    "    compute_metrics = compute_metrics)\n",
    "\n",
    "sub_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e303e",
   "metadata": {},
   "source": [
    "## Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e0e57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with standard setup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with hierarchical setup...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text\t\t: mga walang tulog for leni\n",
      "Preprocessed text\t: tulog CANDIDATE\n",
      "\n",
      "##### MULTICLASS CLASSIFIER RESULT #####\n",
      "\tNon-Abusive\n",
      "\n",
      "\n",
      "##### HIERARCHICAL CLASSIFIER RESULT #####\n",
      "\tNon-Abusive\n",
      "\n",
      "\n",
      "Input 'exit' to stop.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"Input 'exit' to stop.\")\n",
    "    intext = input()\n",
    "    if intext == \"exit\":\n",
    "        break;\n",
    "    clear_output(wait=True)\n",
    "    text = [preprocess_phase_1_and_2(intext)]\n",
    "    \n",
    "    # Preprocess text\n",
    "    print(\"Preprocessing text...\")\n",
    "    testing_df = pd.DataFrame([[x, -1] for x in text], columns=[\"text\", \"labels\"])\n",
    "    testing_set = Dataset.from_pandas(testing_df)\n",
    "    testing_set = testing_set.class_encode_column(\"labels\")\n",
    "    testing_set_tokenized = testing_set.map(tokenize_function, batched=True)\n",
    "\n",
    "    testing_set_tokenized = testing_set_tokenized.remove_columns([\"text\"])\n",
    "    testing_set_tokenized.set_format(\"torch\")\n",
    "\n",
    "    # Get predictions from standard setup\n",
    "    print(\"Predicting with standard setup...\")\n",
    "    standard_trainer.evaluate(testing_set_tokenized)\n",
    "    standard_prediction = predictions[0]\n",
    "    \n",
    "    # Get predictions from main hierarchical model\n",
    "    print(\"Predicting with hierarchical setup...\")\n",
    "    main_trainer.evaluate(testing_set_tokenized)\n",
    "    main_prediction = predictions[0]\n",
    "    \n",
    "    # Get predictions from sub hierarchical model\n",
    "    if main_prediction == 1:\n",
    "        sub_trainer.evaluate(testing_set_tokenized)\n",
    "        sub_prediction = predictions[0]\n",
    "        \n",
    "    \n",
    "    print(f'\\nInput text\\t\\t: {intext}')\n",
    "    print(f'Preprocessed text\\t: {text[0]}')\n",
    "    print(\"\\n##### MULTICLASS CLASSIFIER RESULT #####\")\n",
    "    if standard_prediction == 0:\n",
    "        print(\"\\tNon-Abusive\")\n",
    "    elif standard_prediction == 1:\n",
    "        print(\"\\tExplicitly Abusive\")\n",
    "    elif standard_prediction == 2:\n",
    "        print(\"\\tImplicitly Abusive\")\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"##### HIERARCHICAL CLASSIFIER RESULT #####\")\n",
    "    if main_prediction == 0:\n",
    "        print(\"\\tNon-Abusive\")\n",
    "    elif main_prediction == 1:\n",
    "        print(\"\\tAbusive\")\n",
    "        if sub_prediction == 0:\n",
    "            print(\"\\t> Explicitly Abusive\")\n",
    "        elif sub_prediction == 1:\n",
    "            print(\"\\t> Implicitly Abusive\")\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
